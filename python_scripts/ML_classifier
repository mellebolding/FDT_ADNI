import os
import sys

# Absolute :path to the current script
script_dir = os.path.dirname(os.path.abspath(__file__))

# Absolute path to the repo root (one level up from this script)
repo_root = os.path.abspath(os.path.join(script_dir, '..'))

os.chdir(repo_root)

sys.path.insert(0, repo_root)
sys.path.insert(0, os.path.join(repo_root, 'support_files'))
sys.path.insert(0, os.path.join(repo_root, 'DataLoaders'))
results_dir = os.path.join(repo_root, 'Result_plots')
Ceff_sigma_subfolder = os.path.join(results_dir, 'Ceff_sigma_results')
FDT_values_subfolder = os.path.join(results_dir, 'FDT_values')
FDT_parcel_subfolder = os.path.join(results_dir, 'FDT_parcel')
FDT_subject_subfolder = os.path.join(results_dir, 'FDT_sub')
Inorm1_group_subfolder = os.path.join(results_dir, 'Inorm1_group')
Inorm2_group_subfolder = os.path.join(results_dir, 'Inorm2_group')
Inorm1_sub_subfolder = os.path.join(results_dir, 'Inorm1_sub')
Inorm2_sub_subfolder = os.path.join(results_dir, 'Inorm2_sub')
import numpy as np
from functions_FDT_numba_v9 import *
from numba import njit, prange, objmode
from functions_FC_v3 import *
from functions_LinHopf_Ceff_sigma_fit_v6 import LinHopf_Ceff_sigma_fitting_numba
from scipy.linalg import solve_continuous_lyapunov
import pandas as pd
import matplotlib.pyplot as plt
from functions_violinplots_WN3_v0 import plot_violins_HC_MCI_AD
from functions_boxplots_WN3_v0 import plot_boxes_HC_MCI_AD
import p_values as p_values
import statannotations_permutation
from nilearn import surface, datasets, plotting
import nibabel as nib
from matplotlib.cm import ScalarMappable
from matplotlib.colors import Normalize
import ADNI_A
from sklearn.model_selection import LeaveOneOut, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel
from sklearn.pipeline import Pipeline
from sklearn.inspection import permutation_importance
from sklearn.metrics import accuracy_score, classification_report

### Loads data from npz file ######################################
def load_appended_records(filepath, filters=None, verbose=False):
    """
    Loads appended records from an .npz file created by `append_record_to_npz`,
    with optional multi-key filtering.

    Parameters
    ----------
    filepath : str
        Path to the .npz file.
    filters : dict or None
        Dictionary of key-value pairs to match (e.g., {'level': 'group', 'condition': 'COND_A'}).
    verbose : bool
        If True, prints debug info.

    Returns
    -------
    list[dict]
        List of matching records.
    """
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"File '{filepath}' not found.")

    with np.load(filepath, allow_pickle=True) as data:
        if "records" not in data:
            raise KeyError(f"'records' key not found in {filepath}")
        records = list(data["records"])

    if filters:
        records = [
            rec for rec in records
            if all(rec.get(k) == v for k, v in filters.items())
        ]

    if verbose:
        print(f"[load] Loaded {len(records)} matching record(s) from '{filepath}'.")
        if records:
            print(f"[load] Keys in first record: {list(records[0].keys())}")

    return records

def get_field(records, field, filters=None):
    """
    Extract list of values for `field` from records,
    optionally filtering by `filters` dict.
    """
    if filters:
        filtered = [r for r in records if all(r.get(k) == v for k, v in filters.items())]
    else:
        filtered = records
    return [r[field] for r in filtered if field in r]

def append_record_to_npz(folder, filename, **record):
    """
    Appends a record (dict) to a 'records' array in a .npz file located in `folder`.
    Creates the folder and file if they don't exist.

    Parameters
    ----------
    folder : str
        Path to the subfolder where the file will be saved.
    filename : str
        Name of the .npz file (e.g., 'Ceff_sigma_results.npz').
    record : dict
        Arbitrary key-value pairs to store (arrays, strings, numbers, etc.).
    """
    os.makedirs(folder, exist_ok=True)  # ensure subfolder exists
    filepath = os.path.join(folder, filename)

    if os.path.exists(filepath):
        existing_data = dict(np.load(filepath, allow_pickle=True))
        records = list(existing_data.get("records", []))
    else:
        records = []

    records.append(record)
    np.savez(filepath, records=np.array(records, dtype=object))

def load_PET_data(NPARCELLS):
    DL = ADNI_A.ADNI_A(normalizeBurden=False)
    HC_IDs = DL.get_groupSubjects('HC')
    MCI_IDs = DL.get_groupSubjects('MCI')
    AD_IDs = DL.get_groupSubjects('AD')
    HC_ABeta = []
    HC_Tau = []
    MCI_ABeta = []
    MCI_Tau = []
    AD_ABeta = []
    AD_Tau = []
    for subject in HC_IDs:
        data = DL.get_subjectData(subject,printInfo=False)
        HC_ABeta.append(np.vstack(data[subject]['ABeta'])) 
        HC_Tau.append(np.vstack(data[subject]['Tau']))
    for subject in MCI_IDs:
        data = DL.get_subjectData(subject,printInfo=False)
        MCI_ABeta.append(np.vstack(data[subject]['ABeta']))
        MCI_Tau.append(np.vstack(data[subject]['Tau']))
    for subject in AD_IDs:
        data = DL.get_subjectData(subject,printInfo=False)
        AD_ABeta.append(np.vstack(data[subject]['ABeta']))
        AD_Tau.append(np.vstack(data[subject]['Tau']))
    ABeta_burden = [np.array(HC_ABeta)[:,:NPARCELLS,0], np.array(MCI_ABeta)[:,:NPARCELLS,0], np.array(AD_ABeta)[:,:NPARCELLS,0]]
    Tau_burden = [np.array(HC_Tau)[:,:NPARCELLS,0], np.array(MCI_Tau)[:,:NPARCELLS,0], np.array(AD_Tau)[:,:NPARCELLS,0]]
    return ABeta_burden, Tau_burden

# Dictionary mapping parcel indices (1-based) to their names
Parcel_names = {
    1: "Right_V1", 2: "Right_MST", 3: "Right_V6", 4: "Right_V2", 5: "Right_V3", 6: "Right_V4", 7: "Right_V8",
    8: "Right_4", 9: "Right_3b", 10: "Right_FEF", 11: "Right_PEF", 12: "Right_55b", 13: "Right_V3A", 14: "Right_RSC",
    15: "Right_POS2", 16: "Right_V7", 17: "Right_IPS1", 18: "Right_FFC", 19: "Right_V3B", 20: "Right_LO1",
    21: "Right_LO2", 22: "Right_PIT", 23: "Right_MT", 24: "Right_A1", 25: "Right_PSL", 26: "Right_SFL",
    27: "Right_PCV", 28: "Right_STV", 29: "Right_7Pm", 30: "Right_7m", 31: "Right_POS1", 32: "Right_23d",
    33: "Right_v23ab", 34: "Right_d23ab", 35: "Right_31pv", 36: "Right_5m", 37: "Right_5mv", 38: "Right_23c",
    39: "Right_5L", 40: "Right_24dd", 41: "Right_24dv", 42: "Right_7AL", 43: "Right_SCEF", 44: "Right_6ma",
    45: "Right_7Am", 46: "Right_7PL", 47: "Right_7PC", 48: "Right_LIPv", 49: "Right_VIP", 50: "Right_MIP",
    51: "Right_1", 52: "Right_2", 53: "Right_3a", 54: "Right_6d", 55: "Right_6mp", 56: "Right_6v",
    57: "Right_p24pr", 58: "Right_33pr", 59: "Right_a24pr", 60: "Right_p32pr", 61: "Right_a24", 62: "Right_d32",
    63: "Right_8BM", 64: "Right_p32", 65: "Right_10r", 66: "Right_47m", 67: "Right_8Av", 68: "Right_8Ad",
    69: "Right_9m", 70: "Right_8BL", 71: "Right_9p", 72: "Right_10d", 73: "Right_8C", 74: "Right_44",
    75: "Right_45", 76: "Right_47l", 77: "Right_a47r", 78: "Right_6r", 79: "Right_IFJa", 80: "Right_IFJp",
    81: "Right_IFSp", 82: "Right_IFSa", 83: "Right_p9-46v", 84: "Right_46", 85: "Right_a9-46v", 86: "Right_9-46d",
    87: "Right_9a", 88: "Right_10v", 89: "Right_a10p", 90: "Right_10pp", 91: "Right_11l", 92: "Right_13l",
    93: "Right_OFC", 94: "Right_47s", 95: "Right_LIPd", 96: "Right_6a", 97: "Right_i6-8", 98: "Right_s6-8",
    99: "Right_43", 100: "Right_OP4", 101: "Right_OP1", 102: "Right_OP2-3", 103: "Right_52", 104: "Right_RI",
    105: "Right_PFcm", 106: "Right_PoI2", 107: "Right_TA2", 108: "Right_FOP4", 109: "Right_MI", 110: "Right_Pir",
    111: "Right_AVI", 112: "Right_AAIC", 113: "Right_FOP1", 114: "Right_FOP3", 115: "Right_FOP2", 116: "Right_PFt",
    117: "Right_AIP", 118: "Right_EC", 119: "Right_PreS", 120: "Right_H", 121: "Right_ProS", 122: "Right_PeEc",
    123: "Right_STGa", 124: "Right_PBelt", 125: "Right_A5", 126: "Right_PHA1", 127: "Right_PHA3", 128: "Right_STSda",
    129: "Right_STSdp", 130: "Right_STSvp", 131: "Right_TGd", 132: "Right_TE1a", 133: "Right_TE1p", 134: "Right_TE2a",
    135: "Right_TF", 136: "Right_TE2p", 137: "Right_PHT", 138: "Right_PH", 139: "Right_TPOJ1", 140: "Right_TPOJ2",
    141: "Right_TPOJ3", 142: "Right_DVT", 143: "Right_PGp", 144: "Right_IP2", 145: "Right_IP1", 146: "Right_IP0",
    147: "Right_PFop", 148: "Right_PF", 149: "Right_PFm", 150: "Right_PGi", 151: "Right_PGs", 152: "Right_V6A",
    153: "Right_VMV1", 154: "Right_VMV3", 155: "Right_PHA2", 156: "Right_V4t", 157: "Right_FST", 158: "Right_V3CD",
    159: "Right_LO3", 160: "Right_VMV2", 161: "Right_31pd", 162: "Right_31a", 163: "Right_VVC", 164: "Right_25",
    165: "Right_s32", 166: "Right_pOFC", 167: "Right_PoI1", 168: "Right_Ig", 169: "Right_FOP5", 170: "Right_p10p",
    171: "Right_p47r", 172: "Right_TGv", 173: "Right_MBelt", 174: "Right_LBelt", 175: "Right_A4", 176: "Right_STSva",
    177: "Right_TE1m", 178: "Right_PI", 179: "Right_a32pr", 180: "Right_p24",
    181: "Left_V1", 182: "Left_MST", 183: "Left_V6", 184: "Left_V2", 185: "Left_V3", 186: "Left_V4", 187: "Left_V8",
    188: "Left_4", 189: "Left_3b", 190: "Left_FEF", 191: "Left_PEF", 192: "Left_55b", 193: "Left_V3A", 194: "Left_RSC",
    195: "Left_POS2", 196: "Left_V7", 197: "Left_IPS1", 198: "Left_FFC", 199: "Left_V3B", 200: "Left_LO1",
    201: "Left_LO2", 202: "Left_PIT", 203: "Left_MT", 204: "Left_A1", 205: "Left_PSL", 206: "Left_SFL",
    207: "Left_PCV", 208: "Left_STV", 209: "Left_7Pm", 210: "Left_7m", 211: "Left_POS1", 212: "Left_23d",
    213: "Left_v23ab", 214: "Left_d23ab", 215: "Left_31pv", 216: "Left_5m", 217: "Left_5mv", 218: "Left_23c",
    219: "Left_5L", 220: "Left_24dd", 221: "Left_24dv", 222: "Left_7AL", 223: "Left_SCEF", 224: "Left_6ma",
    225: "Left_7Am", 226: "Left_7PL", 227: "Left_7PC", 228: "Left_LIPv", 229: "Left_VIP", 230: "Left_MIP",
    231: "Left_1", 232: "Left_2", 233: "Left_3a", 234: "Left_6d", 235: "Left_6mp", 236: "Left_6v",
    237: "Left_p24pr", 238: "Left_33pr", 239: "Left_a24pr", 240: "Left_p32pr", 241: "Left_a24", 242: "Left_d32",
    243: "Left_8BM", 244: "Left_p32", 245: "Left_10r", 246: "Left_47m", 247: "Left_8Av", 248: "Left_8Ad",
    249: "Left_9m", 250: "Left_8BL", 251: "Left_9p", 252: "Left_10d", 253: "Left_8C", 254: "Left_44",
    255: "Left_45", 256: "Left_47l", 257: "Left_a47r", 258: "Left_6r", 259: "Left_IFJa", 260: "Left_IFJp",
    261: "Left_IFSp", 262: "Left_IFSa", 263: "Left_p9-46v", 264: "Left_46", 265: "Left_a9-46v", 266: "Left_9-46d",
    267: "Left_9a", 268: "Left_10v", 269: "Left_a10p", 270: "Left_10pp", 271: "Left_11l", 272: "Left_13l",
    273: "Left_OFC", 274: "Left_47s", 275: "Left_LIPd", 276: "Left_6a", 277: "Left_i6-8", 278: "Left_s6-8",
    279: "Left_43", 280: "Left_OP4", 281: "Left_OP1", 282: "Left_OP2-3", 283: "Left_52", 284: "Left_RI",
    285: "Left_PFcm", 286: "Left_PoI2", 287: "Left_TA2", 288: "Left_FOP4", 289: "Left_MI", 290: "Left_Pir",
    291: "Left_AVI", 292: "Left_AAIC", 293: "Left_FOP1", 294: "Left_FOP3", 295: "Left_FOP2", 296: "Left_PFt",
    297: "Left_AIP", 298: "Left_EC", 299: "Left_PreS", 300: "Left_H", 301: "Left_ProS", 302: "Left_PeEc",
    303: "Left_STGa", 304: "Left_PBelt", 305: "Left_A5", 306: "Left_PHA1", 307: "Left_PHA3", 308: "Left_STSda",
    309: "Left_STSdp", 310: "Left_STSvp", 311: "Left_TGd", 312: "Left_TE1a", 313: "Left_TE1p", 314: "Left_TE2a",
    315: "Left_TF", 316: "Left_TE2p", 317: "Left_PHT", 318: "Left_PH", 319: "Left_TPOJ1", 320: "Left_TPOJ2",
    321: "Left_TPOJ3", 322: "Left_DVT", 323: "Left_PGp", 324: "Left_IP2", 325: "Left_IP1", 326: "Left_IP0",
    327: "Left_PFop", 328: "Left_PF", 329: "Left_PFm", 330: "Left_PGi", 331: "Left_PGs", 332: "Left_V6A",
    333: "Left_VMV1", 334: "Left_VMV3", 335: "Left_PHA2", 336: "Left_V4t", 337: "Left_FST", 338: "Left_V3CD",
    339: "Left_LO3", 340: "Left_VMV2", 341: "Left_31pd", 342: "Left_31a", 343: "Left_VVC", 344: "Left_25",
    345: "Left_s32", 346: "Left_pOFC", 347: "Left_PoI1", 348: "Left_Ig", 349: "Left_FOP5", 350: "Left_p10p",
    351: "Left_p47r", 352: "Left_TGv", 353: "Left_MBelt", 354: "Left_LBelt", 355: "Left_A4", 356: "Left_STSva",
    357: "Left_TE1m", 358: "Left_PI", 359: "Left_a32pr", 360: "Left_p24", 361: "Right_Thalamus", 362: "Right_Caudate",
    363: "Right_Putamen", 364: "Right_Pallidum", 365: "Right_Hippocampus",
    366: "Right_Amygdala", 367: "Right_Nucleus accumbens", 368: "Right_Ventral diencephalon", 369: "Right_Cerebellar cortex",
    370: "Left_Thalamus", 371: "Left_Caudate", 372: "Left_Putamen", 373: "Left_Pallidum", 374: "Left_Hippocampus", 375: "Left_Amygdala",
    376: "Left_Nucleus accumbens", 377: "Left_Ventral diencephalon", 378: "Left_Cerebellar cortex", 379: "Brainstem"
}

NPARCELLS = 379
NOISE_TYPE = 'hetero'#"HOMO"
A_FITTING = True
all_values = None
all_values_a = None
if A_FITTING:
    all_values_a = load_appended_records(
        filepath=os.path.join(FDT_values_subfolder, f"FDT_values_a{A_FITTING}_N{NPARCELLS}_{NOISE_TYPE}.npz")
    )
    I_norm2_group_a = np.squeeze(np.array(get_field(all_values_a, "I_norm2", filters={"level": "group"})), axis=0)
    X_norm2_group_a = np.squeeze(np.array(get_field(all_values_a, "X_Inorm2", filters={"level": "group"})), axis=0)
    I_norm2_sub_a = np.squeeze(np.array(get_field(all_values_a, "I_norm2", filters={"level": "subject"})), axis=0)
    X_norm2_sub_a = np.squeeze(np.array(get_field(all_values_a, "X_Inorm2", filters={"level": "subject"})), axis=0)
    a_values_group = np.squeeze(get_field(all_values_a, "a", filters={"level": "group"}))
    a_values_sub = np.split(get_field(all_values_a, "a", filters={"level": "subject"})[0], [17, 26], axis=0)
    a_original_group = np.squeeze(np.array(get_field(all_values_a, "original_a", filters={"level": "group"})))
    a_original_sub = get_field(all_values_a, "original_a", filters={"level": "subject"})[0][0]

all_values = load_appended_records(
    filepath=os.path.join(FDT_values_subfolder, f"FDT_values_a{A_FITTING}_N{NPARCELLS}_{NOISE_TYPE}.npz")
)
I_norm2_group = np.squeeze(np.array(get_field(all_values, "I_norm2", filters={"level": "group"})), axis=0)
X_norm2_group = np.squeeze(np.array(get_field(all_values, "X_Inorm2", filters={"level": "group"})), axis=0)
I_norm2_sub = np.squeeze(np.array(get_field(all_values, "I_norm2", filters={"level": "subject"})), axis=0)
X_norm2_sub = np.squeeze(np.array(get_field(all_values, "X_Inorm2", filters={"level": "subject"})), axis=0)
ABeta_burden, Tau_burden = load_PET_data(min(NPARCELLS,360))

df_list = []

for cohort_idx, (AB, Tau, I, X) in enumerate(
        zip(ABeta_burden,
            Tau_burden,
            I_norm2_sub_a[:, :, 0:min(NPARCELLS, 360)],
            X_norm2_sub_a[:, :, 0:min(NPARCELLS, 360)])):

    nsub, nparcel = AB.shape
    I = I[:nsub, :]
    X = X[:nsub, :]

    df = pd.DataFrame({
        "subject": np.repeat([f"C{cohort_idx}_S{i}" for i in range(nsub)], nparcel),
        "parcel": np.tile(np.arange(nparcel), nsub),
        "ABeta_local": AB.ravel(),
        "Tau_local": Tau.ravel(),
        "I_local": I.ravel(),
        "X_local": X.ravel(),
        "cohort": cohort_idx
    })
    df_list.append(df)
df_parcel = pd.concat(df_list, ignore_index=True)
# --- Subject-level PET + FDT parcel features (wide) ---
df_wide = df_parcel.pivot(
    index="subject",
    columns="parcel",
    values=["ABeta_local", "Tau_local", "I_local", "X_local"]
)
df_wide.columns = [f"{feat}_parcel{p}" for feat, p in df_wide.columns]

# --- Subject-level averages + cohort ---
df_means = (
    df_parcel.groupby("subject")
    .agg({
        "ABeta_local": "mean",
        "Tau_local": "mean",
        "I_local": "mean",
        "X_local": "mean",
        "cohort": "first"
    })
    .rename(columns={
        "ABeta_local": "ABeta_local_mean",
        "Tau_local": "Tau_local_mean",
        "I_local": "I_local_mean",
        "X_local": "X_local_mean",
        "cohort": "group"
    })
)

# --- Combine into final subject-level dataframe ---
df_subject = df_wide.merge(df_means, left_index=True, right_index=True).reset_index()

print(df_subject.shape)
print(df_subject.head())



### ML CLASSIFIER
# Idea is to build a script that does the following:
# - Classify HC/MCI/AD using (1) SVM (with top-k F1 features), (2) RF, and (3) using RF+SVM
# - Use (1) empirical PET data (parcel values + global average), (2) FDT metrics (first I, then I+X; again parcel values + global), and (3) both
# - Needs to be configurable and general enough to be used for other datasets as well
# number of PET features: 360+360+1+1 = 722 -> sqrt = 27
# number of FDT features: 379+379+1+1 = 760 -> sqrt = 28
# number of PET+FDT features: 722+760 = 1482 -> sqrt = 39

def ML_classifier(algorithm_type, data_type, validation_type, data, n_features):
    """
    Parameters
    ----------
    algorithm_type : str
        "SVM", "RF", or "RF+SVM"
    data_type : str
        "PET", "FDT", or "BOTH"
    data : pd.DataFrame
        Dataframe with columns: subject, group (0=HC, 1=MCI, 2=AD), and features
    k_features : int
        Number of top features to select for SVM

    Returns
    -------
    None
        Prints classification accuracy and feature importance
    """
    if data_type == "PET":
        feature_cols = [col for col in data.columns if "ABeta" in col or "Tau" in col]
    elif data_type == "FDT":
        feature_cols = [col for col in data.columns if "I_local" in col or "X_local" in col]
    elif data_type == "BOTH":
        feature_cols = [col for col in data.columns if any(f in col for f in ["ABeta", "Tau", "I_local", "X_local"])]
    else:
        raise ValueError("Invalid data_type. Choose 'PET', 'FDT', or 'BOTH'.")

    X = data[feature_cols].values 
    y = data["group"].values

    if algorithm_type == "SVM":
        model = Pipeline([
            ("feature_selection", SelectKBest(f_classif, k=int(np.sqrt(n_features)))),
            ("svm", SVC(kernel="rbf", C=1000, random_state=42))
        ])
    elif algorithm_type == "RF":

        #n_features = X.shape[1]
        max_features = int(np.sqrt(n_features))

        rf_selector = SelectFromModel(
            RandomForestClassifier(n_estimators=500, criterion="entropy", random_state=42),
            max_features=max_features,
            threshold=-np.inf
        )

        rf_classifier = RandomForestClassifier(
            n_estimators=500, criterion="entropy", random_state=42, class_weight="balanced"
        )

        model = Pipeline([
            ("feature_selection", rf_selector),
            ("rf", rf_classifier)
        ])
    elif algorithm_type == "RF+SVM":
        rf = SelectFromModel(
        RandomForestClassifier(n_estimators=500, criterion="entropy", random_state=42),
        max_features=int(np.sqrt(n_features)),  # select top 20 features
        threshold=-np.inf  # allow explicit max_features selection
        )
        model = Pipeline([
            ("feature_selection", rf),
            ("svm", SVC(kernel="rbf", C=1, gamma="scale", class_weight="balanced", random_state=42))
        ])
    else:
        raise ValueError("Invalid algorithm_type. Choose 'SVM', 'RF', or 'RF+SVM'.")

    if validation_type == "LOO":
        from sklearn.model_selection import LeaveOneOut
        loo = LeaveOneOut()
        scores = []
        for train_idx, test_idx in loo.split(X):
            model.fit(X[train_idx], y[train_idx])
            scores.append(model.score(X[test_idx], y[test_idx]))
        print(f"Leave-One-Out CV Accuracy: {np.mean(scores):.4f}")

    elif validation_type == "TEST/TRAIN":
        subjects = data["subject"].unique()
        # Stratified split by cohort at the subject level
        train_subjects, test_subjects = train_test_split(
            subjects, test_size=0.2, random_state=42,
            stratify=data.loc[data["subject"].isin(subjects), "cohort"]
        )
        X_train = data.loc[data["subject"].isin(train_subjects), feature_cols].values
        y_train = data.loc[data["subject"].isin(train_subjects), "cohort"].values
        X_test = data.loc[data["subject"].isin(test_subjects), feature_cols].values
        y_test = data.loc[data["subject"].isin(test_subjects), "cohort"].values
        
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        print("Test Accuracy:", accuracy_score(y_test, y_pred))
        print(classification_report(y_test, y_pred))
    
    else:
        raise ValueError("Invalid validation_type. Choose 'LOO' or 'TEST/TRAIN'.")

    print(f"Leave-One-Out CV Accuracy: {np.mean(scores):.4f}")

ML_classifier("RF+SVM", "BOTH", df_subject.rename(columns={"cohort": "group"}), n_features=1482)
ML_classifier("RF+SVM", "PET", df_subject.rename(columns={"cohort": "group"}), n_features=722)
ML_classifier("RF+SVM", "FDT", df_subject.rename(columns={"cohort": "group"}), n_features=760)

